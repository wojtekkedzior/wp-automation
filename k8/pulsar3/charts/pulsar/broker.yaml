## Pulsar: Broker cluster
## templates/broker-statefulset.yaml
##
broker:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: broker
  replicaCount: 2
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    metrics: ~
    behavior: ~
  # The podManagementPolicy cannot be modified for an existing deployment. If you need to change this value, you will need to manually delete the existing broker StatefulSet and then redeploy the chart.
  podManagementPolicy:
  initContainers: []
  # This is how prometheus discovers this component
  podMonitor:
    enabled: true
    interval: 60s
    scrapeTimeout: 60s
    metricRelabelings:
      # - action: labeldrop
      #   regex: cluster
  # True includes annotation for statefulset that contains hash of corresponding configmap, which will cause pods to restart on configmap change
  restartPodsOnConfigMapChange: false
  ports:
    http: 8080
    https: 8443
    pulsar: 6650
    pulsarssl: 6651
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  probe:
    liveness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
    readiness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
    startup:
      enabled: false
      failureThreshold: 30
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 5
  affinity:
    anti_affinity: true
    anti_affinity_topology_key: kubernetes.io/hostname
    # Set the anti affinity type. Valid values:
    # requiredDuringSchedulingIgnoredDuringExecution - rules must be met for pod to be scheduled (hard) requires at least one node per replica
    # preferredDuringSchedulingIgnoredDuringExecution - scheduler will try to enforce but not guranentee
    type: preferredDuringSchedulingIgnoredDuringExecution
  # set topologySpreadConstraint to deploy pods across different zones
  topologySpreadConstraints: []
  annotations: {}
  tolerations: []
  gracePeriod: 30
  ## Timeout for waiting for zookeeper to become available before starting a broker
  waitZookeeperTimeout: 600
  ## Timeout for waiting for bookkeeper to become available before starting a broker
  waitBookkeeperTimeout: 120
  resources:
    requests:
      memory: 8000Mi
      cpu: 2
    limits:
      memory: 10000Mi
      cpu: 2
  # extraVolumes and extraVolumeMounts allows you to mount other volumes
  # Example Use Case: mount ssl certificates
  # extraVolumes:
  #   - name: ca-certs
  #     secret:
  #       defaultMode: 420
  #       secretName: ca-certs
  # extraVolumeMounts:
  #   - name: ca-certs
  #     mountPath: /certs
  #     readOnly: true
  extraVolumes: []
  extraVolumeMounts: []
  extreEnvs: []
#    - name: POD_NAME
#      valueFrom:
#        fieldRef:
#          apiVersion: v1
#          fieldPath: metadata.name
  ## Broker configmap
  ## templates/broker-configmap.yaml
  ## Keys in broker.conf can be overridden here. Use PULSAR_PREFIX_ to add keys to broker.conf.
  ## In addition, keys in function_worker.yml can be overridden using the PF_ prefix, with _ serving as the key path separator.
  ##
  configData:
    PULSAR_MEM: >
      -Xms1600m -Xmx1900m -XX:MaxDirectMemorySize=8000m
    PULSAR_GC: >
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      -Dio.netty.leakDetectionLevel=disabled
      -Dio.netty.recycler.linkCapacity=1024
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+DoEscapeAnalysis
      -XX:ParallelGCThreads=4
      -XX:ConcGCThreads=4
      -XX:G1NewSizePercent=50
      -XX:+DisableExplicitGC
      -XX:-ResizePLAB
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
    managedLedgerDefaultEnsembleSize: "2"
    managedLedgerDefaultWriteQuorum: "2"
    managedLedgerDefaultAckQuorum: "2"
    enableReplicatedSubscriptions: "true"
    replicatedSubscriptionsSnapshotTimeoutSeconds: "180"
    replicatedSubscriptionsSnapshotFrequencyMillis: "1000"
    managedLedgerMaxLedgerRolloverTimeMinutes: "3"
    managedLedgerMinLedgerRolloverTimeMinutes: "1"
    managedLedgerCursorRolloverTimeInSeconds: "60"
    managedLedgerMaxEntriesPerLedger: "100"
#    systemTopicEnabled: "true"
#    topicLevelPoliciesEnabled: "true"
    # bookkeeperNumberOfChannelsPerBookie: "32"
    # loadBalancerAutoBundleSplitEnabled: "true"
    bookkeeperClientTimeoutInSeconds: "180"
    # bookkeeperClientNumWorkerThreads: "10"
    #numIOThreads: "10"
    managedLedgerCacheSizeMB: "4000"

  ## Add a custom command to the start up process of the broker pods (e.g. update-ca-certificates, jvm commands, etc)
  additionalCommand:
  ## Broker service
  ## templates/broker-service.yaml
  ##
  service:
    # clusterIP can be one of the three, which determines the type of k8s service deployed for broker
    # 1. a valid IPv4 address -> non-headless service, let you select the IPv4 address
    # 2. '' -> non-headless service, k8s picks an IPv4 address
    # 3. 'None' -> headless
    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-clusterip
    clusterIP: "None"
    annotations: {}
  ## Broker PodDisruptionBudget
  ## templates/broker-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1
  ### Broker service account
  ## templates/broker-service-account.yaml
  service_account:
    annotations: {}
      ## You may use the following annotation in order to use EKS IAM Roles for Service Accounts (IRSA)
      # eks.amazonaws.com/role-arn: arn:aws:iam::66666:role/my-iam-role-with-s3-access
  ## Tiered Storage
  ##
  storageOffload: {}
    ## General
    ## =======
    # maxBlockSizeInBytes: "64000000"
    # readBufferSizeInBytes: "1000000"
    ## The following are default values for the cluster. They can be changed
    ## on each namespace.
    # managedLedgerOffloadDeletionLagMs: "14400000"
    # managedLedgerOffloadAutoTriggerSizeThresholdBytes: "-1" # disabled

    ## For AWS S3
    ## ======
    ## Either you must create an IAM account with access to the bucket and
    ## generate keys for that account, or use IAM Roles for Service Accounts (IRSA)
    ## (example on `.Value.broker.service_account.annotations` section above)
    ##
    # driver: aws-s3
    # bucket: <bucket>
    # region: <region>
    ## Secret that stores AWS credentials, using the following command:
    ## ```
    ## kubectl -n pulsar create secret generic \
    ##   --from-literal=AWS_ACCESS_KEY_ID=<AWS ACCESS KEY> \
    ##   --from-literal=AWS_SECRET_ACCESS_KEY=<AWS SECRET KEY> \
    ##   <secret name>
    ## ```
    # secret: <secret name> # [k8s secret name that stores AWS credentials]

    ## For S3 Compatible
    ## =================
    ## Need to create access and secret key for S3 compatible service
    #
    # driver: aws-s3
    # bucket: <bucket>
    # region: <region>
    # serviceEndpoint: host:port
    ## Secret that stores AWS credentials, using the following command:
    ## ```
    ## kubectl -n pulsar create secret generic \
    ##   --from-literal=AWS_ACCESS_KEY_ID=<AWS ACCESS KEY> \
    ##   --from-literal=AWS_SECRET_ACCESS_KEY=<AWS SECRET KEY> \
    ##   <aws secret name>
    ## ```
    # secret: <aws secret name> # [k8s secret name that stores AWS credentials]

    ## For Azure Blob
    ## =================
    ## Need to create an Azure storage account and a blob containter (bucket)
    ## To retrieve key, see https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal#code-try-1
    #
    # driver: azureblob
    # bucket: <bucket>
    # region: <region>
    ## Secret that stores AZURE credentials, using the following command:
    ## ```
    ## kubectl -n pulsar create secret generic \
    ##   --from-literal=AZURE_STORAGE_ACCOUNT=<AZURE STORAGE ACCOUNT> \
    ##   --from-literal=AZURE_STORAGE_ACCESS_KEY=<AZURE STORAGE ACCESS KEY> \
    ##   <azure secret name>
    ## ```
    # secret: <azure secret name> # [k8s secret name that stores AZURE credentials]

    ## For Google Cloud Storage
    ## ====================
    ## You must create a service account that has access to the objects in GCP buckets
    ## and upload its key as a JSON file to a secret.
    ##
    ## 1. Go to https://console.cloud.google.com/iam-admin/serviceaccounts
    ## 2. Select your project.
    ## 3. Create a new service account.
    ## 4. Give the service account permission to access the bucket. For example,
    ##    the "Storage Object Admin" role.
    ## 5. Create a key for the service account and save it as a JSON file.
    ## 6. Save the JSON file in a secret:
    ##       kubectl create secret generic pulsar-gcp-sa-secret \
    ##          --from-file=google-service-account-key.json \
    ##          --namespace pulsar
    ##
    # driver: google-cloud-storage
    # bucket: <bucket>
    # region: <region>
    # gcsServiceAccountSecret: pulsar-gcp-sa-secret # pragma: allowlist secret
    # gcsServiceAccountJsonFile: google-service-account-key.json